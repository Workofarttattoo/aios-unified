"""
Copyright (c) 2025 Joshua Hendricks Cole (DBA: Corporation of Light).
All Rights Reserved. PATENT PENDING.

Comprehensive Validation Suite
==============================

Validates quantum teleportation protocols through:
- Monte Carlo simulations (10,000+ runs)
- Statistical confidence intervals
- Noise robustness analysis
- Comparison with published experimental results
- Error propagation analysis
"""

from typing import Dict, List, Tuple, Optional, Callable
from dataclasses import dataclass, asdict
import numpy as np
from scipy import stats
import logging

logger = logging.getLogger(__name__)


@dataclass
class StatisticalResult:
    """Statistical analysis of validation results."""
    mean: float
    std: float
    median: float
    ci_95_lower: float  # 95% confidence interval
    ci_95_upper: float
    min_val: float
    max_val: float
    num_samples: int
    skewness: float
    kurtosis: float


@dataclass
class ValidationReport:
    """Comprehensive validation report for a protocol."""
    protocol_name: str
    distance_km: float

    # Fidelity statistics
    fidelity_nominal: float
    fidelity_stats: StatisticalResult

    # Noise robustness
    noise_robustness: Dict[str, float]  # noise_type -> min_fidelity

    # Error propagation
    error_budget_allocation: Dict[str, float]
    error_propagation: Dict[str, float]

    # Hardware comparison
    hardware_comparison: Dict[str, float]

    # Pass/fail criteria
    passes_validation: bool
    confidence_level: float
    min_acceptable_fidelity: float

    # Detailed results
    detailed_report: str


class MonteCarloValidator:
    """Monte Carlo simulation validator for quantum protocols."""

    def __init__(self, num_runs: int = 10000):
        """Initialize validator."""
        self.num_runs = num_runs
        self.results: List[float] = []

    def run_simulation(self, protocol_fn: Callable, num_runs: Optional[int] = None) -> List[float]:
        """Run Monte Carlo simulation."""
        if num_runs is None:
            num_runs = self.num_runs

        results = []
        for i in range(num_runs):
            try:
                fidelity = protocol_fn()
                if 0.0 <= fidelity <= 1.0:
                    results.append(fidelity)
            except Exception as e:
                logger.warning(f"Simulation {i} failed: {e}")
                continue

        self.results = results
        return results

    def analyze_results(self) -> StatisticalResult:
        """Analyze simulation results with confidence intervals."""
        if not self.results:
            raise ValueError("No simulation results available")

        results = np.array(self.results)

        # Calculate confidence interval using bootstrap
        ci_95 = self._bootstrap_ci(results, confidence=0.95)

        return StatisticalResult(
            mean=float(np.mean(results)),
            std=float(np.std(results)),
            median=float(np.median(results)),
            ci_95_lower=float(ci_95[0]),
            ci_95_upper=float(ci_95[1]),
            min_val=float(np.min(results)),
            max_val=float(np.max(results)),
            num_samples=len(results),
            skewness=float(stats.skew(results)),
            kurtosis=float(stats.kurtosis(results))
        )

    @staticmethod
    def _bootstrap_ci(data: np.ndarray, confidence: float = 0.95,
                      num_bootstrap: int = 10000) -> Tuple[float, float]:
        """Calculate confidence interval via bootstrap."""
        bootstrap_means = []
        for _ in range(num_bootstrap):
            sample = np.random.choice(data, size=len(data), replace=True)
            bootstrap_means.append(np.mean(sample))

        bootstrap_means = np.array(bootstrap_means)
        alpha = 1 - confidence
        lower = np.percentile(bootstrap_means, alpha / 2 * 100)
        upper = np.percentile(bootstrap_means, (1 - alpha / 2) * 100)

        return (lower, upper)


class NoiseRobustnessAnalyzer:
    """Analyzes protocol robustness under various noise conditions."""

    def __init__(self):
        """Initialize analyzer."""
        self.noise_models = {
            "amplitude_damping": self._simulate_amplitude_damping,
            "phase_damping": self._simulate_phase_damping,
            "depolarizing": self._simulate_depolarizing,
            "thermal": self._simulate_thermal
        }

    def analyze_robustness(self, base_fidelity: float, num_qubits: int = 1) -> Dict[str, float]:
        """Analyze robustness across noise models."""
        results = {}

        for noise_type, noise_fn in self.noise_models.items():
            min_fidelity = 1.0
            for error_rate in np.linspace(0.001, 0.01, 10):
                fidelity = noise_fn(base_fidelity, error_rate, num_qubits)
                min_fidelity = min(min_fidelity, fidelity)
            results[noise_type] = min_fidelity

        return results

    @staticmethod
    def _simulate_amplitude_damping(fidelity: float, decay_rate: float, num_qubits: int) -> float:
        """Simulate amplitude damping (T1 relaxation)."""
        # F_damped = F_0 * (1 - decay_rate)^num_qubits
        degradation = (1 - decay_rate) ** num_qubits
        return fidelity * degradation

    @staticmethod
    def _simulate_phase_damping(fidelity: float, dephase_rate: float, num_qubits: int) -> float:
        """Simulate phase damping (T2 dephasing)."""
        # F_damped = F_0 * (1 - dephase_rate/2)^num_qubits
        degradation = (1 - dephase_rate / 2) ** num_qubits
        return fidelity * degradation

    @staticmethod
    def _simulate_depolarizing(fidelity: float, error_rate: float, num_qubits: int) -> float:
        """Simulate depolarizing noise."""
        # F_depol = F_0 * (1 - 4*error_rate/3)^num_qubits for single-qubit
        # Multi-qubit: compound effect
        degradation = (1 - 4 * error_rate / 3) ** num_qubits
        return max(0.0, fidelity * degradation)

    @staticmethod
    def _simulate_thermal(fidelity: float, thermal_noise: float, num_qubits: int) -> float:
        """Simulate thermal noise effects."""
        # Thermal noise reduces fidelity proportionally
        degradation = 1.0 - (thermal_noise * num_qubits * 0.1)
        return max(0.0, fidelity * degradation)


class ErrorPropagationAnalyzer:
    """Analyzes error propagation through quantum circuits."""

    @staticmethod
    def allocate_error_budget(distance_km: float, num_qubits: int,
                             target_fidelity: float) -> Dict[str, float]:
        """Allocate error budget across components."""
        # Error budget allocation: 30% photon loss, 40% gates, 20% measurement, 10% decoherence
        total_allowable_error = 1.0 - target_fidelity
        num_gates = 2 * num_qubits  # Estimate gates needed

        allocation = {
            "photon_loss": total_allowable_error * 0.30,
            "gate_errors": total_allowable_error * 0.40 / max(1, num_gates),
            "measurement_errors": total_allowable_error * 0.20,
            "decoherence": total_allowable_error * 0.10
        }

        return allocation

    @staticmethod
    def propagate_errors(error_sources: Dict[str, float], num_gates: int) -> Dict[str, float]:
        """Propagate individual errors through circuit."""
        propagated = {}

        # Gate errors compound
        gate_error = error_sources.get("gate_error", 0.0001)
        propagated["cumulative_gate_error"] = 1.0 - (1.0 - gate_error) ** num_gates

        # Measurement error
        propagated["measurement_error"] = error_sources.get("measurement_error", 0.001)

        # Photon loss (independent)
        propagated["photon_loss"] = error_sources.get("photon_loss", 0.01)

        # Combined fidelity impact
        total_error = (propagated["cumulative_gate_error"] +
                      propagated["measurement_error"] +
                      propagated["photon_loss"])
        propagated["total_fidelity_loss"] = min(1.0, total_error)
        propagated["estimated_fidelity"] = max(0.0, 1.0 - total_error)

        return propagated


class PublishedResultsComparison:
    """Compare results with published experimental data."""

    # Published results from major quantum hardware providers (Oct 2025)
    PUBLISHED_RESULTS = {
        "Google_Willow_Bell_State": {
            "two_qubit_fidelity": 0.9975,
            "publication": "Google Quantum AI (2025)",
            "notes": "Latest achieved fidelity"
        },
        "IBM_Heron_Bell_State": {
            "two_qubit_fidelity": 0.9945,
            "publication": "IBM Quantum (2025)",
            "notes": "Heron processor"
        },
        "IonQ_Harmony_Bell_State": {
            "two_qubit_fidelity": 0.999,
            "publication": "IonQ (2025)",
            "notes": "Trapped ion platform"
        },
        "Rigetti_Aspen_Bell_State": {
            "two_qubit_fidelity": 0.98,
            "publication": "Rigetti (2025)",
            "notes": "Superconducting qubits"
        }
    }

    @classmethod
    def compare_with_published(cls, measured_fidelity: float,
                              protocol_type: str = "Bell State") -> Dict:
        """Compare measured fidelity with published results."""
        comparison = {
            "measured_fidelity": measured_fidelity,
            "benchmark_results": {},
            "performance_ranking": ""
        }

        # Collect benchmark fidelities
        benchmarks = []
        for result_name, result_data in cls.PUBLISHED_RESULTS.items():
            if protocol_type in result_name:
                fidelity = result_data["two_qubit_fidelity"]
                benchmarks.append((result_name, fidelity, result_data["publication"]))

        # If no exact match, use all published results
        if not benchmarks:
            for result_name, result_data in cls.PUBLISHED_RESULTS.items():
                fidelity = result_data["two_qubit_fidelity"]
                benchmarks.append((result_name, fidelity, result_data["publication"]))

        benchmarks.sort(key=lambda x: x[1], reverse=True)

        # Compare
        for rank, (name, fidelity, publication) in enumerate(benchmarks, 1):
            comparison["benchmark_results"][name] = {
                "fidelity": fidelity,
                "publication": publication,
                "vs_measured": fidelity - measured_fidelity,
                "rank": rank
            }

        # Determine ranking
        if benchmarks and measured_fidelity >= benchmarks[0][1]:
            comparison["performance_ranking"] = "EXCEEDS STATE-OF-THE-ART ğŸ†"
        elif benchmarks and measured_fidelity >= benchmarks[0][1] * 0.95:
            comparison["performance_ranking"] = "Competitive with leading hardware"
        elif benchmarks and measured_fidelity >= benchmarks[-1][1]:
            comparison["performance_ranking"] = "Below state-of-the-art but viable"
        else:
            comparison["performance_ranking"] = "Requires improvement"

        return comparison


class ComprehensiveValidator:
    """Comprehensive validation combining all validation techniques."""

    def __init__(self):
        """Initialize validator."""
        self.mc_validator = MonteCarloValidator(num_runs=10000)
        self.noise_analyzer = NoiseRobustnessAnalyzer()
        self.error_propagation = ErrorPropagationAnalyzer()
        self.published_comparison = PublishedResultsComparison()

    def validate_protocol(
        self,
        protocol_name: str,
        protocol_fn: Callable,
        distance_km: float,
        num_qubits: int,
        min_acceptable_fidelity: float = 0.90,
        num_monte_carlo: int = 10000
    ) -> ValidationReport:
        """Comprehensive protocol validation."""

        # 1. Run Monte Carlo simulation
        logger.info(f"Running Monte Carlo validation ({num_monte_carlo} runs)...")
        self.mc_validator.num_runs = num_monte_carlo
        mc_results = self.mc_validator.run_simulation(protocol_fn)
        fidelity_stats = self.mc_validator.analyze_results()

        # 2. Analyze noise robustness
        logger.info("Analyzing noise robustness...")
        nominal_fidelity = fidelity_stats.mean
        noise_robustness = self.noise_analyzer.analyze_robustness(nominal_fidelity, num_qubits)

        # 3. Analyze error propagation
        logger.info("Analyzing error propagation...")
        error_budget = self.error_propagation.allocate_error_budget(
            distance_km, num_qubits, min_acceptable_fidelity
        )
        error_propagation = self.error_propagation.propagate_errors(
            {"gate_error": 0.001, "measurement_error": 0.005, "photon_loss": 0.01},
            num_gates=2 * num_qubits
        )

        # 4. Compare with published results
        logger.info("Comparing with published results...")
        hardware_comparison = self.published_comparison.compare_with_published(nominal_fidelity)

        # 5. Determine validation pass/fail
        passes = (
            fidelity_stats.mean >= min_acceptable_fidelity and
            fidelity_stats.ci_95_lower >= min_acceptable_fidelity * 0.95
        )

        confidence_level = (fidelity_stats.mean - min_acceptable_fidelity) / (1.0 - min_acceptable_fidelity)
        confidence_level = max(0.0, min(1.0, confidence_level))

        # Generate detailed report
        detailed_report = self._generate_report(
            protocol_name, distance_km, fidelity_stats, noise_robustness,
            error_budget, error_propagation, hardware_comparison,
            passes, confidence_level, min_acceptable_fidelity
        )

        return ValidationReport(
            protocol_name=protocol_name,
            distance_km=distance_km,
            fidelity_nominal=nominal_fidelity,
            fidelity_stats=fidelity_stats,
            noise_robustness=noise_robustness,
            error_budget_allocation=error_budget,
            error_propagation=error_propagation,
            hardware_comparison=hardware_comparison,
            passes_validation=passes,
            confidence_level=confidence_level,
            min_acceptable_fidelity=min_acceptable_fidelity,
            detailed_report=detailed_report
        )

    @staticmethod
    def _generate_report(protocol_name: str, distance_km: float,
                        fidelity_stats: StatisticalResult,
                        noise_robustness: Dict,
                        error_budget: Dict,
                        error_propagation: Dict,
                        hardware_comparison: Dict,
                        passes: bool,
                        confidence_level: float,
                        min_fidelity: float) -> str:
        """Generate detailed validation report."""

        report = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘           COMPREHENSIVE VALIDATION REPORT - {protocol_name.upper()}
â•‘           Distance: {distance_km} km | Min Acceptable Fidelity: {min_fidelity*100:.1f}%
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Š MONTE CARLO SIMULATION RESULTS (10,000 runs)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  Mean Fidelity:           {fidelity_stats.mean*100:.2f}%
  Std Deviation:           {fidelity_stats.std*100:.2f}%
  Median:                  {fidelity_stats.median*100:.2f}%
  95% CI:                  [{fidelity_stats.ci_95_lower*100:.2f}%, {fidelity_stats.ci_95_upper*100:.2f}%]
  Min Observed:            {fidelity_stats.min_val*100:.2f}%
  Max Observed:            {fidelity_stats.max_val*100:.2f}%
  Distribution Skewness:   {fidelity_stats.skewness:.3f}
  Distribution Kurtosis:   {fidelity_stats.kurtosis:.3f}

ğŸ›¡ï¸  NOISE ROBUSTNESS ANALYSIS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"""
        for noise_type, min_fid in noise_robustness.items():
            status = "âœ…" if min_fid >= min_fidelity else "âš ï¸"
            report += f"  {status} {noise_type.upper():25s}: {min_fid*100:6.2f}% (min)\n"

        report += f"""

âš™ï¸  ERROR BUDGET ALLOCATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"""
        for component, budget in error_budget.items():
            report += f"  â€¢ {component.replace('_', ' ').title():30s}: {budget*100:6.3f}%\n"

        report += f"""

â¡ï¸  ERROR PROPAGATION THROUGH CIRCUIT
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"""
        for error_type, value in error_propagation.items():
            if error_type != "estimated_fidelity":
                report += f"  â€¢ {error_type.replace('_', ' ').title():35s}: {value*100:6.3f}%\n"

        report += f"\n  Estimated Fidelity After Errors: {error_propagation.get('estimated_fidelity', 0)*100:.2f}%\n"

        report += f"""

ğŸ† COMPARISON WITH PUBLISHED RESULTS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  Performance Ranking: {hardware_comparison.get('performance_ranking', 'Unknown')}

  Measured Fidelity: {hardware_comparison.get('measured_fidelity', 0)*100:.2f}%

"""
        benchmarks = hardware_comparison.get('benchmark_results', {})
        for result_name, result_data in sorted(benchmarks.items(),
                                               key=lambda x: x[1].get('rank', 99)):
            fidelity = result_data['fidelity']
            vs_measured = result_data['vs_measured']
            symbol = "ğŸ“ˆ" if vs_measured > 0 else "ğŸ“‰"
            report += f"  {symbol} {result_name:40s}: {fidelity*100:.2f}% ({vs_measured:+.2f}%)\n"

        report += f"""

âœ… VALIDATION RESULT
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  Status:              {'PASS âœ…' if passes else 'FAIL âŒ'}
  Confidence Level:    {confidence_level*100:.1f}%
  Meets Requirements:  {fidelity_stats.mean >= min_fidelity}
  CI Lower > Min:      {fidelity_stats.ci_95_lower >= min_fidelity * 0.95}

â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
        return report


def demo_validation_suite():
    """Demonstrate validation suite capabilities."""
    print("\n" + "="*80)
    print("COMPREHENSIVE VALIDATION SUITE DEMO")
    print("="*80 + "\n")

    validator = ComprehensiveValidator()

    # Define a simple protocol simulation function
    def bell_state_protocol():
        """Simulate Bell state creation with realistic noise."""
        # Bell state fidelity depends on gate fidelities
        bell_fidelity = 0.98  # Nominal
        # Add some randomness (realistic variation)
        noise = np.random.normal(0, 0.01)
        return np.clip(bell_fidelity + noise, 0, 1)

    # Run comprehensive validation
    report = validator.validate_protocol(
        protocol_name="Bell State Teleportation",
        protocol_fn=bell_state_protocol,
        distance_km=10,
        num_qubits=2,
        min_acceptable_fidelity=0.90,
        num_monte_carlo=10000
    )

    print(report.detailed_report)
    print("\nâœ… Validation complete!\n")


if __name__ == "__main__":
    demo_validation_suite()
